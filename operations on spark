from pyspark.sql import SparkSession
import getpass
username = getpass.getuser()
spark = SparkSession. \
    builder. \
    config('spark.ui.port','0'). \
    config("spark.sql.warehouse.dir", f"/user/itv000173/warehouse"). \
    enableHiveSupport(). \
    master('yarn'). \
    getOrCreate()


orders_rdd = spark.sparkContext.textFile("/public/trendytech/retail_db/orders/*")

orders_rdd.collect()/orders_rdd.take(5)  ---> collect will show you all the data together once instead of using collect we should use take and use a specific number.
question 1 : Count the orders in each category ?
we will use map transformation beacuse it give one to one output .

mapped_rdd = orders_rdd.map(lambda x:(x.split(","),[3],1))
mapped_rdd.take(10)


OUTPUT:=
(['1', '2013-07-25 00:00:00.0', '11599', 'CLOSED'], [3], 1),
 (['2', '2013-07-25 00:00:00.0', '256', 'PENDING_PAYMENT'], [3], 1),
 (['3', '2013-07-25 00:00:00.0', '12111', 'COMPLETE'], [3], 1),
 (['4', '2013-07-25 00:00:00.0', '8827', 'CLOSED'], [3], 1),
 (['5', '2013-07-25 00:00:00.0', '11318', 'COMPLETE'], [3], 1),
 (['6', '2013-07-25 00:00:00.0', '7130', 'COMPLETE'], [3], 1),
 (['7', '2013-07-25 00:00:00.0', '4530', 'COMPLETE'], [3], 1),
 (['8', '2013-07-25 00:00:00.0', '2911', 'PROCESSING'], [3], 1),
 (['9', '2013-07-25 00:00:00.0', '5657', 'PENDING_PAYMENT'], [3], 1),
 (['10', '2013-07-25 00:00:00.0', '5648', 'PENDING_PAYMENT'], [3], 1)]


Now we want to aggregration , aggregration  will be used for adding the values each category.

reduced_rdd = mapped_rdd.reduceByKey(lambda x,y :x+y)
reduced_rdd.collect()



now we can also do sort i.e sorting in ascending and descending.
reduced_sorted = reduce_rdd.sortBy(lambda x : x[1],false)
reduced_sorted.collect()
